\section{Algorithmen}
probleme übersicht roboter
probleme übersicht gruppe
\subsection{Spielgraph}
\textcolor{red}{To review!}\\
Da der Roboter seine Entscheidungen basierend auf den Enforce-Algorithmen treffen soll, müssen wir einen endlichen
Spielgraphen erstellen. Ein solcher Graph besteht aus Zuständen, in denen der Spieler (in diesem Fall der Roboter)
und Zuständen in denen die Umgebung (in diesem Fall das Kind) einen Zug macht. Ein Zug wird dabei von einer Kante im Graphen
dargestellt.\par

In unserem Fall sind die möglichen Züge die jeweiligen Bewegungsmuster, die den Spielern mitgegeben werden.
Demnach besteht unser Spielgraph $G = (Q, I, Q_0, Q_1, A_0, A_1, \Omega)$, orientiert an den Vorlesungsunterlagen, aus diesen Elementen:
\begin{itemize}
	\item $Q$: Menge aller Zustände
	\item $I \subseteq Q_0$: Menge aller Startzustände
	\item $Q_0  \text{ und }  Q_1$: Teilmengen von $Q$
	\begin{itemize}
		\item $Q_0$: Zustände in denen der Roboter am Zug ist
		\item $Q_1$: Zustände in denen das Kind am Zug ist
	\end{itemize}
	\item $A_0$: Züge des Roboters
	\item $A_1$: Züge des Kindes
	\item $\Omega \subseteq Q$: Zustände, in denen der Roboter auf dem Ziel steht
\end{itemize}

Die Zustände enthalten dabei die Positionsinformation des Roboters und des Kindes, und welcher Spieler am Zug ist.

Um einen Spielgraphen für unser Spielfeld zu erhalten generieren wir also, ausgehend von einem Initialzustand $Q0$, alle möglichen darauf folgenden Transitionen, abhängig davon, ob der daraus entstehende Folgezustand valide ist. Es werden dabei jeweils entweder die Bewegungesmuster des Kindes oder die des Roboters gewählt. Die daraus folgenden Zustände werden in die Datenstruktur mit aufgenommen. Wenn jene zuvor noch nicht bekannt waren, werden auch von diesen Zuständen die Folgezustände generiert, bis keine neuen Zustände mehr generiert werden. 

Da schon der Spielgraph illegale Zustände vermeiden muss, wird hier bereits beachtet, ob der Roboter mit dem Kind kollidiert oder nicht. Die offensichtliche, einfache Lösung, unabhängig davon wer am Zug ist illegale Folgezustände zu verbieten entspräche natürlich nicht der Aufgabenstellung, weil dann auch das Kind dem Roboter aktiv ausweichen würde. Es soll aber der Roboter dem Kind ausweichen, also ggf. auch vorhersehen ob er mit dem Kind kollidiert. Diese Funktionalität wird umgesetzt, indem für alle generierten Folgezustände des Roboters zusätzlich die darauf folgenden Möglichen Züge des Kindes berechnet werden. Enthält diese zweite Menge einen Zustand, in dem das Kind in den Roboter hineinspringt, wird der aktuelle Roboterzustand verworfen, da er zu einem illegalen Zug führen kann.

Wird die Graphengenerierung so umgesetzt erhalten wir bereits einen Roboter, der dem Kind aktiv ausweichen kann, egal welchen Zug das Kind macht. Dies gilt natürlich nur unter der Voraussetzung, dass die angegebenen Bewegungsmuster dies Zulassen. Kann das Kind bspw. sich frei bewegen ($A_1 = {u,d,l,r}$), der Roboter jedoch gar nicht ($A_0 = {e}$), so kann der Roboter offensichtlich auch keine Kollision vermeiden.


\subsection{Enforce}
Auf dem Graphen wird der Enforce-Wert für die Bewegungen des Roboters und des Kindes berechnet. Dabei treten die Aktionen der beiden Beteiligten immer in abwechselnder Reihenfolge ein. Zu erst kommt immer die Bewegung des Roboters, dann die des Kindes. Dieses wiederholt sich, bis der Roboter sein Ziel erreicht hat, oder dieses nicht mehr erreichbar ist, durch einen Fehlerzustand. Letzterer ist zu vermeiden und durch den Enforce-Wert möglichst zu vermeiden. \\
Für die Berechnung wird in zwei verschiedene Algorithmen unterschieden, zum einen Enforce und zum Anderen Enforce+ . Die beiden Algorithmen und ihre Umsetzung werden im folgenden diskutiert. \\
Der Enforce Algorithmus selbst ist das Grundgerüst, welches zu erst aufgebaut wird. Dabei wird vom Zielpunkt ausgegangen, denn der Roboter erreichen soll und Rückwärts der Enforce-Wert für die Knotenpunkte berechnet. Dabei erhält der Zielpunkt selbst denn Wert '0'. Von hier an wird eine Wiederholung eingeleitet die durchgeführt wird, bis der Startzustand erreicht ist, oder als nicht erreichbar gilt. \\
In der Wiederholung wird zuerst der Enforce Wert um einen hoch gezählt und alle Kanten die in einen Knoten gehen den man im vorherigen Schritt betrachtet hat zurückverfolgt. Dabei stößt man auf zwei Arten von Knoten, es wird zwischen einem Knoten unterschieden in dem der Roboter am Zug ist und Einem in dem das Kind am Zug ist. Dieses bildet die klassische zwei Spieler Partie ab. \\
Im Falle des Roboter-Knotenpunktes wird geschaut ob dieser schon einen Enforce-Wert hat, sollte dieses nicht so sein, wird der jetzige Wert eingetragen. Dieses bedeutet, da der Roboter immer den bestmöglichen Weg nimmt, dass es entweder schon einen vorhandenen besseren Weg gibt, oder jetzt einer gefunden wurde der zum Ziel führt in maximal Enforce-Wert Schritten. Dieses spiegelt sich in der Existenz eines möglichen Weges wieder, der für den Roboter wichtig ist. \\
Solch ein Verhalten ist bei den Knotenpunkten des Kindes nicht zu erwarten. Das Kind stellt die Umgebung da, welche bei jeder möglichen Bewegung dennoch den Roboter nicht vom erreichen des Ziels abhalten soll. Die Umgebung soll mit ihren Möglichkeiten den Roboter möglichst stark einschränken oder gänzlich verhindern, wobei der Roboter dennoch einen Weg finden soll. Dieses zeigt auf, dass wenn ein Knotenpunkt gefunden wird, jeder Nachfolger dieses Knoten schon einen Enforce-Wert haben muss, damit die Umgebung keine schlechte Alternative treffen kann. Somit ist bei einem betrachten eines solchen Knotenpunktes es möglich, dass entweder alle anderen Nachfolger schon betrachtet worden sind, dann wird der jetzige Enforce-Wert eingetragen, oder es sind nicht alle betrachtet worden, folglich wird kein Wert eingetragen. Es kann nicht sein, dass schon ein Wert vorhanden ist, da sonst der vorherige Knoten schon betrachtet worden wäre, welches nicht möglich ist. Durch diese Unterscheidung liegt in diesem Fall der Allquantor vor. Jede Aktion die die Umgebung wählen kann, muss dennoch zu einem Enforce-Wert führen. Dieses folgert, dass das Kind im schlimmsten Fall das Gewinnen des Roboters nur maximal herauszögern kann, jedoch nie gänzlich verhindern. \\
Damit erfüllt der Enforce Algorithmus die Aufgabenstellung, dass der Roboter zu einem Zeitpunkt in der Zukunft das Ziel erreichen wird. Jedoch ist noch nicht gegeben, dass er dieses auch mehrfach erreichen kann. Für diesen Teil ist der Enforce+ Algorithmus zuständig. Dieser erweitert den Enforce Algorithmus indem er nicht beim erreichen des Startknotens stoppt, sondern darüber hinaus noch das Zielfeld vom Zielfeld selbst. Dieses meint, dass es mindestens einen Zyklus geben muss vom Zielfeld zu sich selbst, welcher weiterhin mit Enforce-Werten gefüllt ist. Dieses bedeutet Enforce+ prüft wenn es an einen Vorgängerknoten geht, ob dieser schon einen Enforce-Wert hat und ob dieser Enforce-0 ist. Sollte dieser gefunden werden gibt es einen Zyklus. Enforce+ geht dabei wie Enforce selbst vor, nur hat es diese beiden Abbruchbedingungen, welche er finden muss. Weiterhin liegt die Schwierigkeit darin, ob der Zielknoten ein Kind-Knoten ist oder ein Roboter-Knoten. Denn vom Kindknoten gilt der Allquantor, welches bedeutet, dass es nicht zwingen nur einen Zyklus geben muss, sondern jeder der ausgehenden Pfade einen bilden muss. Sollte dieses nicht der Fall sein, kann die Umgebung, in diesem Falle das Kind, den Roboter vom erneuten gewinnen abhalten. \\
Somit kann Enforce+ den gesamten begehbaren Raum abdecken und sicher stellen, dass das Kind den Roboter niemals davon abhält das Ziel in der Zukunft erneut zu erreichen. \\
\subsection{Controller}
Es folgen Notizen.
\subsection{Controller}\textcolor{red}{To review!}\\
Der Controller ist für die Schnittstelle zwischen dem Graphen, inklusive des Enforce-Graphen und der GUI zuständig. Er bezieht aus der GUI die Positionierungen des Kindes und des Roboters, sowie die Standorte der Mauern, Zielfelder und Batterien. Er wird mit Start des Programms aus der GUI-Umgebung gestartet. Daraufhin initialisiert er den Graphen und berechnet mit hinzunahme der aktuellen Position des Kindes sowie des Roboters,den Enforce-Graphen. Sollte festgestellt werden, dass es keine Lösung für das Problem gibt, terminiert das Programm. Sollte dies nicht der Fall sein wird der Enforce-Graph für den ersten und die fortlaufenden Schritte des Roboters genutzt. Die Schritte des Kindes wird auch aus diesem Graphen bezogen, dabei bekommt der Controller die möglichen Moves aus dem Graphen und wählt zufällig einen. Mit der Hinzunahme des Energie-Problems wird der Controller um die Aufgabe erweitert das Energie-Level zu überprüfen. Sollte der Worst Case Pfad zum nächsten Zielfeld, zuzüglich des Weges zum nächstbesten Energiefeld, größer als das Energie Level des Roboter sein, bewegt sich der Roboter zum nächsten Batteriefeld. Sollte auch dies nicht möglich sein terminiert das Programm. Der Weg zum nächstbesten Batteriefeld, ist der geringste Weg mit dem geringsten Aufwand aus den Worst Case Pfaden aller Batteriefeld. Als Energiefeld gelten die Felder um die Batterie.


%Es folgen Notizen.
%
%Um zu bestimmen ob ein bestimmtes Ziel (Zielfläche oder Batterie) erreicht werden kann, wird eine Metrik benötigt,
%die die Distanz angibt. Je nachdem, ob der Controller optimistisch oder pessimistisch handeln soll kann dies der kürzeste
%Pfad durch den Graphen sein (der Enforce-Wert wird mit jeder Transition so stark verringert wir möglich)
%, oder der längste Pfad, bei dem der Enforce-Wert jedoch mit jeder Transition um mindestens 1 sinkt.