\section{Logik}
Ziel ist es, mit der Logik in dem Controller die Büchi-Akzeptanz zu erreichen. 
Um Büchi-Akzeptanz zu erreichen beginnen wir zunächst mit der 1-Akzeptanz. Diese besagt in unserem Fall, dass alle Ziel in der richtigen Reihenfolge einmal durchlaufen werden. Um nun auf die Büchi-Akzeptanz zu kommen, wird nach erreichen des letzten Zieles als nächstes Ziel das erste Ziel ausgewählt. Dadurch ist der Roboter in einer Endlosschleife, in der er immer alle Ziele versucht zu durchlaufen.
\subsection{Spielgraph}
\textcolor{red}{To review!}\\
Da der Roboter seine Entscheidungen basierend auf den Enforce-Algorithmen treffen soll, müssen wir einen endlichen
Spielgraphen erstellen. Ein solcher Graph besteht aus Zuständen, in denen der Spieler (in diesem Fall der Roboter)
und Zuständen in denen die Umgebung (in diesem Fall das Kind) einen Zug macht. Ein Zug wird dabei von einer Kante im Graphen
dargestellt.\par

In unserem Fall sind die möglichen Züge die jeweiligen Bewegungsmuster, die den Spielern mitgegeben werden.
Demnach besteht unser Spielgraph $G = (Q, I, Q_0, Q_1, A_0, A_1, \Omega)$, orientiert an den Vorlesungsunterlagen, aus diesen Elementen:
\begin{itemize}
	\item $Q$: Menge aller Zustände
	\item $I \subseteq Q_0$: Menge aller Startzustände
	\item $Q_0  \text{ und }  Q_1$: Teilmengen von $Q$
	\begin{itemize}
		\item $Q_0$: Zustände in denen der Roboter am Zug ist
		\item $Q_1$: Zustände in denen das Kind am Zug ist
	\end{itemize}
	\item $A_0$: Züge des Roboters
	\item $A_1$: Züge des Kindes
	\item $\Omega \subseteq Q$: Zustände, in denen der Roboter auf dem Ziel steht
\end{itemize}

Die Zustände enthalten dabei die Positionsinformation des Roboters und des Kindes, und welcher Spieler am Zug ist.

Um einen Spielgraphen für unser Spielfeld zu erhalten generieren wir also, ausgehend von einem Initialzustand $Q0$, alle möglichen darauf folgenden Transitionen, abhängig davon, ob der daraus entstehende Folgezustand valide ist. Es werden dabei jeweils entweder die Bewegungesmuster des Kindes oder die des Roboters gewählt. Die daraus folgenden Zustände werden in die Datenstruktur mit aufgenommen. Wenn jene zuvor noch nicht bekannt waren, werden auch von diesen Zuständen die Folgezustände generiert, bis keine neuen Zustände mehr generiert werden. 

Da schon der Spielgraph illegale Zustände vermeiden muss, wird hier bereits beachtet, ob der Roboter mit dem Kind kollidiert oder nicht. Die offensichtliche, einfache Lösung, unabhängig davon wer am Zug ist illegale Folgezustände zu verbieten entspräche natürlich nicht der Aufgabenstellung, weil dann auch das Kind dem Roboter aktiv ausweichen würde. Es soll aber der Roboter dem Kind ausweichen, also ggf. auch vorhersehen ob er mit dem Kind kollidiert. Diese Funktionalität wird umgesetzt, indem für alle generierten Folgezustände des Roboters zusätzlich die darauf folgenden Möglichen Züge des Kindes berechnet werden. Enthält diese zweite Menge einen Zustand, in dem das Kind in den Roboter hineinspringt, wird der aktuelle Roboterzustand verworfen, da er zu einem illegalen Zug führen kann.

Wird die Graphengenerierung so umgesetzt erhalten wir bereits einen Roboter, der dem Kind aktiv ausweichen kann, egal welchen Zug das Kind macht. Dies gilt natürlich nur unter der Voraussetzung, dass die angegebenen Bewegungsmuster dies Zulassen. Kann das Kind bspw. sich frei bewegen ($A_1 = {u,d,l,r}$), der Roboter jedoch gar nicht ($A_0 = {e}$), so kann der Roboter offensichtlich auch keine Kollision vermeiden.


\subsection{Enforce}
Auf dem Graphen wird der Enforce-Wert für die Bewegungen des Roboters und des Kindes berechnet. Dabei treten die Aktionen der beiden Beteiligten immer in abwechselnder Reihenfolge ein. Zu erst kommt immer die Bewegung des Roboters, dann die des Kindes. Dieses wiederholt sich, bis der Roboter sein Ziel erreicht hat, oder dieses nicht mehr erreichbar ist, durch einen Fehlerzustand. Letzterer ist zu vermeiden und durch den Enforce-Wert möglichst zu vermeiden. \\
Für die Berechnung wird in zwei verschiedene Algorithmen unterschieden, zum einen Enforce und zum Anderen Enforce+ . Die beiden Algorithmen und ihre Umsetzung werden im folgenden diskutiert. \\
Der Enforce Algorithmus selbst ist das Grundgerüst, welches zu erst aufgebaut wird. Dabei wird vom Zielpunkt ausgegangen, denn der Roboter erreichen soll und Rückwärts der Enforce-Wert für die Knotenpunkte berechnet. Dabei erhält der Zielpunkt selbst denn Wert '0'. Von hier an wird eine Wiederholung eingeleitet die durchgeführt wird, bis der Startzustand erreicht ist, oder als nicht erreichbar gilt. \\
In der Wiederholung wird zuerst der Enforce Wert um einen hoch gezählt und alle Kanten die in einen Knoten gehen den man im vorherigen Schritt betrachtet hat zurückverfolgt. Dabei stößt man auf zwei Arten von Knoten, es wird zwischen einem Knoten unterschieden in dem der Roboter am Zug ist und Einem in dem das Kind am Zug ist. Dieses bildet die klassische zwei Spieler Partie ab. \\
Im Falle des Roboter-Knotenpunktes wird geschaut ob dieser schon einen Enforce-Wert hat, sollte dieses nicht so sein, wird der jetzige Wert eingetragen. Dieses bedeutet, da der Roboter immer den bestmöglichen Weg nimmt, dass es entweder schon einen vorhandenen besseren Weg gibt, oder jetzt einer gefunden wurde der zum Ziel führt in maximal Enforce-Wert Schritten. Dieses spiegelt sich in der Existenz eines möglichen Weges wieder, der für den Roboter wichtig ist. \\
Solch ein Verhalten ist bei den Knotenpunkten des Kindes nicht zu erwarten. Das Kind stellt die Umgebung da, welche bei jeder möglichen Bewegung dennoch den Roboter nicht vom erreichen des Ziels abhalten soll. Die Umgebung soll mit ihren Möglichkeiten den Roboter möglichst stark einschränken oder gänzlich verhindern, wobei der Roboter dennoch einen Weg finden soll. Dieses zeigt auf, dass wenn ein Knotenpunkt gefunden wird, jeder Nachfolger dieses Knoten schon einen Enforce-Wert haben muss, damit die Umgebung keine schlechte Alternative treffen kann. Somit ist bei einem betrachten eines solchen Knotenpunktes es möglich, dass entweder alle anderen Nachfolger schon betrachtet worden sind, dann wird der jetzige Enforce-Wert eingetragen, oder es sind nicht alle betrachtet worden, folglich wird kein Wert eingetragen. Es kann nicht sein, dass schon ein Wert vorhanden ist, da sonst der vorherige Knoten schon betrachtet worden wäre, welches nicht möglich ist. Durch diese Unterscheidung liegt in diesem Fall der Allquantor vor. Jede Aktion die die Umgebung wählen kann, muss dennoch zu einem Enforce-Wert führen. Dieses folgert, dass das Kind im schlimmsten Fall das Gewinnen des Roboters nur maximal herauszögern kann, jedoch nie gänzlich verhindern. \\
Damit erfüllt der Enforce Algorithmus die Aufgabenstellung, dass der Roboter zu einem Zeitpunkt in der Zukunft das Ziel erreichen wird. Jedoch ist noch nicht gegeben, dass er dieses auch mehrfach erreichen kann. Für diesen Teil ist der Enforce+ Algorithmus zuständig. Dieser erweitert den Enforce Algorithmus indem er nicht beim erreichen des Startknotens stoppt, sondern darüber hinaus noch das Zielfeld vom Zielfeld selbst. Dieses meint, dass es mindestens einen Zyklus geben muss vom Zielfeld zu sich selbst, welcher weiterhin mit Enforce-Werten gefüllt ist. Dieses bedeutet Enforce+ prüft wenn es an einen Vorgängerknoten geht, ob dieser schon einen Enforce-Wert hat und ob dieser Enforce-0 ist. Sollte dieser gefunden werden gibt es einen Zyklus. Enforce+ geht dabei wie Enforce selbst vor, nur hat es diese beiden Abbruchbedingungen, welche er finden muss. Weiterhin liegt die Schwierigkeit darin, ob der Zielknoten ein Kind-Knoten ist oder ein Roboter-Knoten. Denn vom Kindknoten gilt der Allquantor, welches bedeutet, dass es nicht zwingen nur einen Zyklus geben muss, sondern jeder der ausgehenden Pfade einen bilden muss. Sollte dieses nicht der Fall sein, kann die Umgebung, in diesem Falle das Kind, den Roboter vom erneuten gewinnen abhalten. \\
Somit kann Enforce+ den gesamten begehbaren Raum abdecken und sicher stellen, dass das Kind den Roboter niemals davon abhält das Ziel in der Zukunft erneut zu erreichen. \\
\subsection{Controller}\textcolor{red}{To review!}\\
Der Controller ist die Schnittstelle zwischen dem Graphen, inklusive der Enforce-Werte und der GUI. Er bezieht aus der GUI die Position des Kindes und des Roboters, sowie die Standorte der Mauern, Zielfelder und Batterien. Er wird mit Start des Programms aus der GUI-Umgebung aufgerufen. Daraufhin initialisiert er den Graphen und berechnet mit hinzunahme der aktuellen Position des Kindes sowie des Roboters, den enforceten Graphen, abhängig vom aktuellen Ziel. Sollte festgestellt werden, dass es keine Lösung für das Problem gibt, terminiert das Programm. Sollte dies nicht der Fall sein wird der Enforce-Graph für den ersten und die fortlaufenden Schritte des Roboters genutzt. Die Schritte des Kindes wird auch aus diesem Graphen bezogen, dabei bekommt der Controller die möglichen Moves aus dem Graphen und wählt zufällig einen. Mit der Hinzunahme des Energie-Problems wird der Controller um die Aufgabe erweitert, das Energie-Level zu überprüfen. Sollte der Worst-Case-Pfad zum nächsten Zielfeld, zuzüglich des Weges zum nächstbesten Energiefeld, größer als das Energie Level des Roboter sein, bewegt sich der Roboter zum nächsten Batteriefeld. Sollte auch dies nicht möglich sein terminiert das Programm. Der Weg zum nächstbesten Batteriefeld, ist der geringste Weg mit dem geringsten Aufwand aus den Worst-Case-Pfaden aller Batteriefeld. Als Energiefeld gelten die Felder um die Batterie.

Zur Berechnung der Pfade wird also die WorstCasePath Metrik festgelegt. Diese berechnet sich aus der Summe der Anzahl von Bewegungen die entstehen, wenn der Roboter von seinem aktuellen Zustand in den Zielzustand navigiert. Dabei nimmt der Roboter immer den Weg der zum geringsten Enforce-Wert führt. Das Kind nimmt auf diesem Pfad allerdings immer die Transitionen, die zu dem Zustand mit dem höchsten Enforce-Wert führen. Die Summe der Anzahlen der Bewegungen, die der Roboter auf diesem Pfad macht, ist dann der WorstCasePath ($WCPath(target)$).

Der Ablauf des Controllers ist in Algorithmus~\autoref{alg:controllerPseudo} vereinfacht dargestellt.

\begin{algorithm}[t]
	\caption{Pseudocode des Controllers}\label{alg:controllerPseudo}
	\begin{algorithmic}[1]
		\Procedure{findClosestBattery}{$graphs, position$}
		
		\Return Feld um Batterien, das am nächsten an der gegebenen Position ist
		\EndProcedure
		\\
		\Procedure{findReachableBatteries}{$robotPosition, childPosition$}
		
		\Return Batterien, die vom Roboter erreichbar sind
		\EndProcedure
		\\
		\Procedure{isSolveable}{$graph, robotPosition, childPosition$}
			\State $state \gets graph.findState(robotPosition, childPosition)$
			
			\Return $state.enforceValue != -1$
		\EndProcedure
		\\
		
		\Procedure{Controller}{$gameField, targetFields, batteryFields$}
			\State $graph \gets generateGraph(gameField)$
			\Comment{Spielgraphen generieren}
			\State $batteryGraphs \gets enforceGraphs(graph, batteryFields)$
			\Comment{Graphen für 9 Felder um die Batterien enforcen, um diese als Ziel zu berechnen/verwenden}
			\\
			\While{true}
				\ForAll{targetField $in$ targetFields}
					\State $enforcedGraph \gets enforceGraph(graph, targetField.position)$ 
					
					\Comment{Spielgraph auf aktuelles Ziel enforcen}
					\State $activeGraph \gets enforcedGraph$
					\\
					\While{$targetField.position \neq robot.position \land isSolveable(enforcedGraph, robot.position, child.position)$}
					
					\State $robotToTargetCost \gets enforcedGraph.WCPath(robot.position, child.position)$
					\State $targetToBatteryCost \gets findClosestBattery(targetField.position, batteryGraphs)$
					\\
					\If{$(targetToBatteryCost + robotToTargetCost) > robot.energy$}
						\State $reachableBatteries \gets findReachableBatteries(robot.position, child.position)$
						\State $targetBattery \gets findClosestBattery(reachableBatteries, targetField.position)$
						\State $activeGraph \gets targetBattery.graph$
					\EndIf
					\\
					\State $robot.makeMove(activeGraph.getBestRobotMove(robot.position, child.position))$
					\State $child.makeMove(activeGraph.getRandomChildMove(robot.position, child.position))$
					
					\EndWhile
				\EndFor
			\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Optimierung}
Die Optimierung des Programmes wäre an verschiedenen Stellen möglich, jedoch aufgrund der Aufgabenstellung nicht implementierbar. Ein möglicher Aspekt wird im folgenden vorgestellt, der sich eigenen würde in Hinsicht auf die Aufgabenstellung. Dabei wird das Abfahren von der Ladestation betrachtet. \\
Im Ausgangszustand schaut der Roboter in jedem Schritt wie weit er es zum Ziel hat und wie lange er bis zur nächsten Ladestation brauchen würde. Sollte die Ladestation dabei im schlimmsten Falle gerade noch zu erreichen sein, fährt er zu dieser. Sonst versucht er immer zu aller erst das Ziel zu erreichen und dann sich aufzuladen. \\
Nimmt man auf dieser Basis eine Ladestation an die hinter dem Roboter liegt und zwei oder mehr ab zu fahrende Ziele vor dem Roboter an, so kann man die Optimierungsmöglichkeit festmachen. In der Annahme hat der Roboter nur noch 50\% geladen und kann damit maximal das erste Ziel erreichen, ohne aufladen zu müssen. Sollte er diese Möglichkeit also wählen, fährt er zum ersten Ziel um von diesem den Weg zurück zu fahren um die Ladestation zu erreichen, um anschließend die weiteren Ziele abzuarbeiten. \\
Würde der Roboter an gleicher Stelle prüfen, ob es besser wäre für den Ladungsverbrauch und die Geschwindigkeit der Abarbeitung, dass er erst zur Ladestation fährt und dann alle Ziele auf einmal abarbeiten könnte, so würde er sich mindestens einmal die Strecke zwischen Ladestation und Zielen sparen. \\
Es ist schnell zu sehen, dass solch eine Optimierung mit Einbezug des Kindes zu deutlichen Veränderungen führen kann, da der maximale Weg oft nicht der genommene ist. Dieses beruht auf der Zufälligkeit des Kindes. Weiterhin ist in der Aufgabenstellung vermerkt, dass der Roboter sein Ziel in der Zukunft irgendwann erreichen muss, somit auch zeitliche Umwege vollständig akzeptabel sind. Dieses Beispiel verdeutlicht nur, dass deutliche Optimierungen möglich wären, besonders in Hinsicht auf die Abarbeitungszeit. \\
Eine weitere mögliche Optimierung liegt in der Berechnung des Kindes. Auch wenn dieses zufällig sich bewegt, sind die Bewegungen dennoch von Anfang an gegeben. Somit kommt es häufig vor, dass das Kind sich nur in einem Teil des Raumes aufhält, oder durch Ladestationen und Wände gar dazu gezwungen wird. Wenn der Roboter dieses mit einbezieht, könnte er den längsten Weg in den Kind-freien Zonen deutlich weiter senken. \\
Weitere Optimierungsmöglichkeiten sollen in diesem Exkurs nicht weiter behandelt werden.


%Es folgen Notizen.
%
%Um zu bestimmen ob ein bestimmtes Ziel (Zielfläche oder Batterie) erreicht werden kann, wird eine Metrik benötigt,
%die die Distanz angibt. Je nachdem, ob der Controller optimistisch oder pessimistisch handeln soll kann dies der kürzeste
%Pfad durch den Graphen sein (der Enforce-Wert wird mit jeder Transition so stark verringert wir möglich)
%, oder der längste Pfad, bei dem der Enforce-Wert jedoch mit jeder Transition um mindestens 1 sinkt.